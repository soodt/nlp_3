{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soodt/nlp_3/blob/main/Copy_of_CSE_143_A3_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyeebuhZdJb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "f8b89797-3cb6-426c-a1b7-10386fc3a765"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import os\n",
        "import sys\n",
        "print(os.listdir('/content/gdrive/My Drive/CSE-143-A3'))\n",
        "sys.path.append('/content/gdrive/My Drive/CSE-143-A3')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-229c33cf41de>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/CSE-143-A3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "% cd '/content/gdrive/My Drive/CSE-143-A3'"
      ],
      "metadata": {
        "id": "C8w1fT2zNMvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHdbUbDpdTBp"
      },
      "source": [
        "import random\n",
        "from conlleval import evaluate as conllevaluate\n",
        "\n",
        "directory = '/content/gdrive/My Drive/CSE-143-A3'\n",
        "\n",
        "def decode(input_length, tagset, score):\n",
        "    \"\"\"\n",
        "    Compute the highest scoring sequence according to the scoring function.\n",
        "    :param input_length: int. number of tokens in the input including <START> and <STOP>\n",
        "    :param tagset: Array of strings, which are the possible tags.  Does not have <START>, <STOP>\n",
        "    :param score: function from current_tag (string), previous_tag (string), i (int) to the score.  i=0 points to\n",
        "        <START> and i=1 points to the first token. i=input_length-1 points to <STOP>\n",
        "    :return: Array strings of length input_length, which is the highest scoring tag sequence including <START> and <STOP>\n",
        "    \"\"\"\n",
        "    # Look at the function compute_score for an example of how the tag sequence should be scored\n",
        "    #raise NotImplementedError(\"Complete Viterbi Decoding Here!\")\n",
        "     # Define special tokens\n",
        "    START_TAG = \"<START>\"\n",
        "    STOP_TAG = \"<STOP>\"\n",
        "\n",
        "    # Initialize dynamic programming tables\n",
        "    viterbi = {}\n",
        "    backpointer = {}\n",
        "\n",
        "    # Initialization\n",
        "    viterbi[0] = {START_TAG: 0}\n",
        "    backpointer[0] = {START_TAG: None}\n",
        "\n",
        "    # Recursion\n",
        "    for i in range(1, input_length):\n",
        "        viterbi[i] = {}\n",
        "        backpointer[i] = {}\n",
        "        for cur_tag in (tagset if i < input_length - 1 else [STOP_TAG]):\n",
        "            max_score = float('-inf')\n",
        "            max_tag = None\n",
        "            for pre_tag in (tagset if i > 1 else [START_TAG]):\n",
        "                cur_score = viterbi[i - 1][pre_tag] + score(cur_tag, pre_tag, i)\n",
        "                if cur_score > max_score:\n",
        "                    max_score = cur_score\n",
        "                    max_tag = pre_tag\n",
        "            viterbi[i][cur_tag] = max_score\n",
        "            backpointer[i][cur_tag] = max_tag\n",
        "\n",
        "    # Termination\n",
        "    best_path = [None] * input_length\n",
        "    best_path[input_length - 1] = STOP_TAG\n",
        "\n",
        "    # Backtracking\n",
        "    for i in range(input_length - 2, -1, -1):\n",
        "        best_path[i] = backpointer[i + 1][best_path[i + 1]]\n",
        "\n",
        "    return best_path\n",
        "\n",
        "def compute_score(tag_seq, input_length, score):\n",
        "    \"\"\"\n",
        "    Computes the total score of a tag sequence\n",
        "    :param tag_seq: Array of String of length input_length. The tag sequence including <START> and <STOP>\n",
        "    :param input_length: Int. input length including the padding <START> and <STOP>\n",
        "    :param score: function from current_tag (string), previous_tag (string), i (int) to the score.  i=0 points to\n",
        "        <START> and i=1 points to the first token. i=input_length-1 points to <STOP>\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    total_score = 0\n",
        "    for i in range(1, input_length):\n",
        "        total_score += score(tag_seq[i], tag_seq[i - 1], i)\n",
        "    return total_score\n",
        "\n",
        "\n",
        "def compute_features(tag_seq, input_length, features):\n",
        "    \"\"\"\n",
        "    Compute f(xi, yi)\n",
        "    :param tag_seq: [tags] already padded with <START> and <STOP>\n",
        "    :param input_length: input length including the padding <START> and <STOP>\n",
        "    :param features: func from token index to FeatureVector\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    feats = FeatureVector({})\n",
        "    for i in range(1, input_length):\n",
        "        feats.times_plus_equal(1, features.compute_features(tag_seq[i], tag_seq[i - 1], i))\n",
        "    return feats\n",
        "\n",
        "\n",
        "def sgd(training_size, epochs, gradient, parameters, training_observer):\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "    :param training_size: int. Number of examples in the training set\n",
        "    :param epochs: int. Number of epochs to run SGD for\n",
        "    :param gradient: func from index (int) in range(training_size) to a FeatureVector of the gradient\n",
        "    :param parameters: FeatureVector.  Initial parameters.  Should be updated while training\n",
        "    :param training_observer: func that takes epoch and parameters.  You can call this function at the end of each\n",
        "           epoch to evaluate on a dev set and write out the model parameters for early stopping.\n",
        "    :return: final parameters\n",
        "    \"\"\"\n",
        "    # Look at the FeatureVector object.  You'll want to use the function times_plus_equal to update the\n",
        "    # parameters.\n",
        "    # To implement early stopping you can call the function training_observer at the end of each epoch.\n",
        "    i = 0\n",
        "\n",
        "    while i < epochs:\n",
        "        print('i='+str(i))\n",
        "        data_indices = [ i for i in range(training_size) ]\n",
        "        random.shuffle(data_indices)\n",
        "        counter = 0\n",
        "        for t in data_indices:\n",
        "            if counter % 1000 == 0:\n",
        "                print('Item '+str(counter))\n",
        "            parameters.times_plus_equal(-1, gradient(t))\n",
        "            counter += 1\n",
        "        i += 1\n",
        "        training_observer(i, parameters)\n",
        "    return\n",
        "\n",
        "\n",
        "def train(data, feature_names, tagset, epochs):\n",
        "    \"\"\"\n",
        "    Trains the model on the data and returns the parameters\n",
        "    :param data: Array of dictionaries representing the data.  One dictionary for each data point (as created by the\n",
        "        make_data_point function).\n",
        "    :param feature_names: Array of Strings.  The list of feature names.\n",
        "    :param tagset: Array of Strings.  The list of tags.\n",
        "    :param epochs: Int. The number of epochs to train\n",
        "    :return: FeatureVector. The learned parameters.\n",
        "    \"\"\"\n",
        "    parameters = FeatureVector({})\n",
        "\n",
        "    def perceptron_gradient(i):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the Perceptron loss for example i\n",
        "        :param i: Int\n",
        "        :return: FeatureVector\n",
        "        \"\"\"\n",
        "        inputs = data[i]\n",
        "        input_len = len(inputs['tokens'])\n",
        "        gold_labels = inputs['gold_tags']\n",
        "        features = Features(inputs, feature_names)\n",
        "\n",
        "        def score(cur_tag, pre_tag, i):\n",
        "            return parameters.dot_product(features.compute_features(cur_tag, pre_tag, i))\n",
        "\n",
        "        tags = decode(input_len, tagset, score)\n",
        "        fvector = compute_features(tags, input_len, features)           # Add the predicted features\n",
        "        #print('Input:', inputs)        # helpful for debugging\n",
        "        #print(\"Predicted Feature Vector:\", fvector.fdict)\n",
        "        #print(\"Predicted Score:\", parameters.dot_product(fvector))\n",
        "        fvector.times_plus_equal(-1, compute_features(gold_labels, input_len, features))    # Subtract the features for the gold labels\n",
        "        #print(\"Gold Labels Feature Vector: \", compute_features(gold_labels, input_len, features).fdict)\n",
        "        #print(\"Gold Labels Score:\", parameters.dot_product(compute_features(gold_labels, input_len, features)))\n",
        "        return fvector\n",
        "\n",
        "    def training_observer(epoch, parameters):\n",
        "        \"\"\"\n",
        "        Evaluates the parameters on the development data, and writes out the parameters to a 'model.iter'+epoch and\n",
        "        the predictions to 'ner.dev.out'+epoch.\n",
        "        :param epoch: int.  The epoch\n",
        "        :param parameters: Feature Vector.  The current parameters\n",
        "        :return: Double. F1 on the development data\n",
        "        \"\"\"\n",
        "        dev_data = read_data('ner.dev')\n",
        "        (_, _, f1) = evaluate(dev_data, parameters, feature_names, tagset)\n",
        "        write_predictions('ner.dev.out'+str(epoch), dev_data, parameters, feature_names, tagset)\n",
        "        parameters.write_to_file('model.iter'+str(epoch))\n",
        "        return f1\n",
        "\n",
        "    return sgd(len(data), epochs, perceptron_gradient, parameters, training_observer)\n",
        "\n",
        "\n",
        "def predict(inputs, input_len, parameters, feature_names, tagset):\n",
        "    \"\"\"\n",
        "\n",
        "    :param inputs:\n",
        "    :param input_len:\n",
        "    :param parameters:\n",
        "    :param feature_names:\n",
        "    :param tagset:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    features = Features(inputs, feature_names)\n",
        "\n",
        "    def score(cur_tag, pre_tag, i):\n",
        "        return parameters.dot_product(features.compute_features(cur_tag, pre_tag, i))\n",
        "\n",
        "    return decode(input_len, tagset, score)\n",
        "\n",
        "\n",
        "def make_data_point(sent):\n",
        "    \"\"\"\n",
        "        Creates a dictionary from String to an Array of Strings representing the data.  The dictionary items are:\n",
        "        dic['tokens'] = Tokens padded with <START> and <STOP>\n",
        "        dic['pos'] = POS tags padded with <START> and <STOP>\n",
        "        dic['NP_chunk'] = Tags indicating noun phrase chunks, padded with <START> and <STOP>\n",
        "        dic['gold_tags'] = The gold tags padded with <START> and <STOP>\n",
        "    :param sent: String.  The input CoNLL format string\n",
        "    :return: Dict from String to Array of Strings.\n",
        "    \"\"\"\n",
        "    dic = {}\n",
        "    sent = [s.strip().split() for s in sent]\n",
        "    dic['tokens'] = ['<START>'] + [s[0] for s in sent] + ['<STOP>']\n",
        "    dic['pos'] = ['<START>'] + [s[1] for s in sent] + ['<STOP>']\n",
        "    dic['NP_chunk'] = ['<START>'] + [s[2] for s in sent] + ['<STOP>']\n",
        "    dic['gold_tags'] = ['<START>'] + [s[3] for s in sent] + ['<STOP>']\n",
        "    return dic\n",
        "\n",
        "def read_data(filename):\n",
        "    \"\"\"\n",
        "    Reads the CoNLL 2003 data into an array of dictionaries (a dictionary for each data point).\n",
        "    :param filename: String\n",
        "    :return: Array of dictionaries.  Each dictionary has the format returned by the make_data_point function.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with open(filename, 'r') as f:\n",
        "        sent = []\n",
        "        for line in f.readlines():\n",
        "            if line.strip():\n",
        "                sent.append(line)\n",
        "            else:\n",
        "                data.append(make_data_point(sent))\n",
        "                sent = []\n",
        "        data.append(make_data_point(sent))\n",
        "\n",
        "    return data\n",
        "\n",
        "def write_predictions(out_filename, all_inputs, parameters, feature_names, tagset):\n",
        "    \"\"\"\n",
        "    Writes the predictions on all_inputs to out_filename, in CoNLL 2003 evaluation format.\n",
        "    Each line is token, pos, NP_chuck_tag, gold_tag, predicted_tag (separated by spaces)\n",
        "    Sentences are separated by a newline\n",
        "    The file can be evaluated using the command: python conlleval.py < out_file\n",
        "    :param out_filename: filename of the output\n",
        "    :param all_inputs:\n",
        "    :param parameters:\n",
        "    :param feature_names:\n",
        "    :param tagset:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with open(out_filename, 'w', encoding='utf-8') as f:\n",
        "        for inputs in all_inputs:\n",
        "            input_len = len(inputs['tokens'])\n",
        "            tag_seq = predict(inputs, input_len, parameters, feature_names, tagset)\n",
        "            for i, tag in enumerate(tag_seq[1:-1]):  # deletes <START> and <STOP>\n",
        "                f.write(' '.join([inputs['tokens'][i+1], inputs['pos'][i+1], inputs['NP_chunk'][i+1], inputs['gold_tags'][i+1], tag])+'\\n') # i + 1 because of <START>\n",
        "            f.write('\\n')\n",
        "\n",
        "def evaluate(data, parameters, feature_names, tagset):\n",
        "    \"\"\"\n",
        "    Evaluates precision, recall, and F1 of the tagger compared to the gold standard in the data\n",
        "    :param data: Array of dictionaries representing the data.  One dictionary for each data point (as created by the\n",
        "        make_data_point function)\n",
        "    :param parameters: FeatureVector.  The model parameters\n",
        "    :param feature_names: Array of Strings.  The list of features.\n",
        "    :param tagset: Array of Strings.  The list of tags.\n",
        "    :return: Tuple of (prec, rec, f1)\n",
        "    \"\"\"\n",
        "    all_gold_tags = [ ]\n",
        "    all_predicted_tags = [ ]\n",
        "    for inputs in data:\n",
        "        all_gold_tags.extend(inputs['gold_tags'][1:-1])  # deletes <START> and <STOP>\n",
        "        input_len = len(inputs['tokens'])\n",
        "        all_predicted_tags.extend(predict(inputs, input_len, parameters, feature_names, tagset)[1:-1]) # deletes <START> and <STOP>\n",
        "    return conllevaluate(all_gold_tags, all_predicted_tags)\n",
        "\n",
        "\n",
        "def test_decoder():\n",
        "    # See\n",
        "\n",
        "    tagset = ['NN', 'VB']     # make up our own tagset\n",
        "\n",
        "    def score_wrap(cur_tag, pre_tag, i):\n",
        "        retval = score(cur_tag, pre_tag, i)\n",
        "        print('Score('+cur_tag+','+pre_tag+','+str(i)+') returning '+str(retval))\n",
        "        return retval\n",
        "\n",
        "    def score(cur_tag, pre_tag, i):\n",
        "        if i == 0:\n",
        "            print(\"ERROR: Don't call score for i = 0 (that points to <START>, with nothing before it)\")\n",
        "        if i == 1:\n",
        "            if pre_tag != '<START>':\n",
        "                print(\"ERROR: Previous tag should be <START> for i = 1. Previous tag = \"+pre_tag)\n",
        "            if cur_tag == 'NN':\n",
        "                return 6\n",
        "            if cur_tag == 'VB':\n",
        "                return 4\n",
        "        if i == 2:\n",
        "            if cur_tag == 'NN' and pre_tag == 'NN':\n",
        "                return 4\n",
        "            if cur_tag == 'NN' and pre_tag == 'VB':\n",
        "                return 9\n",
        "            if cur_tag == 'VB' and pre_tag == 'NN':\n",
        "                return 5\n",
        "            if cur_tag == 'VB' and pre_tag == 'VB':\n",
        "                return 0\n",
        "        if i == 3:\n",
        "            if cur_tag != '<STOP>':\n",
        "                print('ERROR: Current tag at i = 3 should be <STOP>. Current tag = '+cur_tag)\n",
        "            if pre_tag == 'NN':\n",
        "                return 1\n",
        "            if pre_tag == 'VB':\n",
        "                return 1\n",
        "\n",
        "    predicted_tag_seq = decode(4, tagset, score_wrap)\n",
        "    print('Predicted tag sequence should be = <START> VB NN <STOP>')\n",
        "    print('Predicted tag sequence = '+' '.join(predicted_tag_seq))\n",
        "    print(\"Score of ['<START>','VB','NN','<STOP>'] = \"+str(compute_score(['<START>','VB','NN','<STOP>'], 4, score)))\n",
        "    print('Max score should be = 14')\n",
        "    print('Max score = '+str(compute_score(predicted_tag_seq, 4, score)))\n",
        "\n",
        "\n",
        "def main_predict(data_filename, model_filename):\n",
        "    \"\"\"\n",
        "    Main function to make predictions.\n",
        "    Loads the model file and runs the NER tagger on the data, writing the output in CoNLL 2003 evaluation format to data_filename.out\n",
        "    :param data_filename: String\n",
        "    :param model_filename: String\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    data = read_data(data_filename)\n",
        "    parameters = FeatureVector({})\n",
        "    parameters.read_from_file(model_filename)\n",
        "\n",
        "    tagset = ['B-PER', 'B-LOC', 'B-ORG', 'B-MISC', 'I-PER', 'I-LOC', 'I-ORG', 'I-MISC', 'O']\n",
        "    feature_names = ['tag', 'prev_tag', 'current_word']\n",
        "\n",
        "    write_predictions(data_filename+'.out', data, parameters, feature_names, tagset)\n",
        "    evaluate(data, parameters, feature_names, tagset)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def main_train():\n",
        "    \"\"\"\n",
        "    Main function to train the model\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    print('Reading training data')\n",
        "    train_data = read_data('ner.train')\n",
        "    #train_data = read_data('ner.train')[1:1] # if you want to train on just one example\n",
        "\n",
        "    tagset = ['B-PER', 'B-LOC', 'B-ORG', 'B-MISC', 'I-PER', 'I-LOC', 'I-ORG', 'I-MISC', 'O']\n",
        "    feature_names = ['tag', 'prev_tag', 'current_word']\n",
        "\n",
        "    print('Training...')\n",
        "    parameters = train(train_data, feature_names, tagset, epochs=10)\n",
        "    print('Training done')\n",
        "    dev_data = read_data('ner.dev')\n",
        "    evaluate(dev_data, parameters, feature_names, tagset)\n",
        "    test_data = read_data('ner.test')\n",
        "    evaluate(test_data, parameters, feature_names, tagset)\n",
        "    parameters.write_to_file('model')\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "class Features(object):\n",
        "    def __init__(self, inputs, feature_names):\n",
        "        \"\"\"\n",
        "        Creates a Features object\n",
        "        :param inputs: Dictionary from String to an Array of Strings.\n",
        "            Created in the make_data_point function.\n",
        "            inputs['tokens'] = Tokens padded with <START> and <STOP>\n",
        "            inputs['pos'] = POS tags padded with <START> and <STOP>\n",
        "            inputs['NP_chunk'] = Tags indicating noun phrase chunks, padded with <START> and <STOP>\n",
        "            inputs['gold_tags'] = DON'T USE! The gold tags padded with <START> and <STOP>\n",
        "        :param feature_names: Array of Strings.  The list of features to compute.\n",
        "        \"\"\"\n",
        "        self.feature_names = feature_names\n",
        "        self.inputs = inputs\n",
        "\n",
        "    def compute_features(self, cur_tag, pre_tag, i):\n",
        "        \"\"\"\n",
        "        Computes the local features for the current tag, the previous tag, and position i\n",
        "        :param cur_tag: String.  The current tag.\n",
        "        :param pre_tag: String.  The previous tag.\n",
        "        :param i: Int. The position\n",
        "        :return: FeatureVector\n",
        "        \"\"\"\n",
        "        feats = FeatureVector({})\n",
        "        if 'tag' in self.feature_names:\n",
        "            feats.times_plus_equal(1, FeatureVector({'t='+cur_tag: 1}))\n",
        "        if 'prev_tag' in self.feature_names:\n",
        "            feats.times_plus_equal(1, FeatureVector({'ti='+cur_tag+\"+ti-1=\"+pre_tag: 1}))\n",
        "        if 'current_word' in self.feature_names:\n",
        "            feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'+w='+self.inputs['tokens'][i]: 1}))\n",
        "        return feats\n",
        "\n",
        "class FeatureVector(object):\n",
        "\n",
        "    def __init__(self, fdict):\n",
        "        self.fdict = fdict\n",
        "\n",
        "    def times_plus_equal(self, scalar, v2):\n",
        "        \"\"\"\n",
        "        self += scalar * v2\n",
        "        :param scalar: Double\n",
        "        :param v2: FeatureVector\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        for key, value in v2.fdict.items():\n",
        "            self.fdict[key] = scalar * value + self.fdict.get(key, 0)\n",
        "\n",
        "\n",
        "    def dot_product(self, v2):\n",
        "        \"\"\"\n",
        "        Computes the dot product between self and v2.  It is more efficient for v2 to be the smaller vector (fewer\n",
        "        non-zero entries).\n",
        "        :param v2: FeatureVector\n",
        "        :return: Int\n",
        "        \"\"\"\n",
        "        retval = 0\n",
        "        for key, value in v2.fdict.items():\n",
        "            retval += value * self.fdict.get(key, 0)\n",
        "        return retval\n",
        "\n",
        "    def write_to_file(self, filename):\n",
        "        \"\"\"\n",
        "        Writes the feature vector to a file.\n",
        "        :param filename: String\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        print('Writing to ' + filename)\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            for key, value in self.fdict.items():\n",
        "                f.write('{} {}\\n'.format(key, value))\n",
        "\n",
        "\n",
        "    def read_from_file(self, filename):\n",
        "        \"\"\"\n",
        "        Reads a feature vector from a file.\n",
        "        :param filename: String\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        self.fdict = {}\n",
        "        with open(filename, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                txt = line.split()\n",
        "                self.fdict[txt[0]] = float(txt[1])\n",
        "\n",
        "#test_decoder()  # Uncomment to test the decoder on a simple example (see https://classes.soe.ucsc.edu/cse143/Winter20/assignments/A3_Debug_Example.pdf)\n",
        "main_predict('ner.dev', 'model.simple')  # Uncomment to predict on 'dev.ner' using the model 'model.simple' (need to implement 'decode' function)\n",
        "#main_train()    # Uncomment to train a model (need to implement 'sgd' function)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oInyKo1pfTW2"
      },
      "source": [
        "To sort the model weights for easy viewing, you can use Unix commands:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHEtAu_LfmPG"
      },
      "source": [
        "!cat \"/content/gdrive/My Drive/CSE-143-A3/model\" | awk '{print $2, $1}' | sort -gr > \"/content/gdrive/My Drive/CSE-143-A3/model.sorted.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bS4d4Acfp1S"
      },
      "source": [
        "The file `model.sorted.txt` will be viewable in your Google Drive folder."
      ]
    }
  ]
}